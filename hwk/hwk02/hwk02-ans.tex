\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[top = 1.0in, left = 1.75in, right = 0.75in, bottom = 0.75in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}

\usepackage[mode = buildnew]{standalone}
\usepackage{tikz}

\usepackage[explicit]{titlesec}
\titleformat{\section}[runin]{\bfseries}{}{0em}{
    \llap{
        \smash{
            \begin{tabularx}{0.75in}[t]{@{}l@{\hskip0.4em}>{\raggedright}X@{\hskip\marginparsep}}
                #1 
            \end{tabularx}
        }
    }
}[\leavevmode\hspace*{\dimexpr-\fontdimen2\font-\fontdimen3\font+0.25em}]

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\renewcommand{\headrulewidth}{0pt}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1\\}
\newcommand{\mycolaba}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1}

%' ============================================================================================================================================================
%' ============================================================================================================================================================

\begin{document}

\newcommand{\mytitle}{Homework 2}
\newcommand{\myauthor}{Aiden Kenny}
\newcommand{\myclass}{STAT GR5203: Probability}
\newcommand{\myschool}{Columbia University}
\newcommand{\mydate}{October 12, 2020}
\begin{flushright}
    \textbf{\mytitle}\\[0.5em]
    \textsl{\myauthor}\\
    \textsl{\myclass}\\
    \textsl{\myschool}\\
    \textsl{\mydate}
\end{flushright} \vspace{1em}

%' ============================================================================================================================================================
\section{Question 1} \noindent
Let \(X\) have a pdf of \(f(x) = c x^2\) for \(0 \le x \le 1\) and \(f(x) = 0\) elsewhere. 
\begin{itemize}
    \item[(a)] For this to be a valid pdf, it must integrate to \(1\) over the support. So 
    \begin{align*}
        1 = \int_0^1 c x^2 \;\mathrm{d}x = \left. \frac{c x^3}{3} \right|_0^1 = \frac{c}{3},
    \end{align*}
    which leads to \(c = 3\). So \(f(x) = 3x^2\) for \(0 \le x \le 1\). 
    \item[(b)] The cdf is given by 
    \begin{align*}
        F(x) = \int_{-\infty}^x f(t) \;\mathrm{d}t = \int_0^x 3t^2 \;\mathrm{d}t = \left. t^3 \right|_0^x = x^3
    \end{align*}
    for \(0 \le x \le 1\). We also have \(F(x) = 0\) when \(x < 0\) and \(F(x) = 1\) when \(x > 1\).
    \item[(c)] We have 
    \begin{align*}
        \mathrm{Pr}\left(\frac{1}{10} \le X \le \frac{1}{2} \right) 
        = F\left( \frac{1}{2} \right) - F \left( \frac{1}{10} \right)
        = \frac{1}{2^3} - \frac{1}{10^3} = \frac{31}{250}.
    \end{align*}
\end{itemize}

%' ============================================================================================================================================================
\section{Question 2} \noindent
Two discrete random variables \(X\) and \(Y\) are jointly distributed. 
\begin{itemize}
    \item[(a)] The marginal pmf for \(X\) is obtained by summing over every value of \(Y\) for each value of \(X\).
    For example, to find the marginal probability that \(X=1\), we have \(f_X(1) = 0.10 + 0.05 + 0.02 + 0.02 = 0.19\). 
    The other values are obtained in the same way. Finding the marginal pmf for \(Y\) is done the exact same way, and 
    it turns out that \(f_X(j) = f_Y(j)\) for \(j = \{1,2,3,4\}\); they are both found in Table \ref{q02-tab}.
    \item[(b)] \(X\) and \(Y\) are not independent. For two random variables to be independent, we need \(f(x,y) = f_X(x)\cdot f_Y(y)\)
    for all possible \((x,y)\) pairs. Here we have \(f(1,1) = 0.10\) and \(f_X(1)\cdot f_Y(1) = 0.19^2 \neq f(1,1)\), meaing 
    \(X\) and \(Y\) are dependent. 
    \item[(c)] To find the conditional pmf of \(X\) given that \(Y=1\), we take each value of \(f(x,1)\) and divide by \(f_Y(1)\), 
    i.e. \(f_{X|Y}(x|1) = f(x,1) / f_Y(1)\). For example, we have \(f_{X|Y}(1|1) = 0.10 / 0.19 = 10/19\). Finding the conditional pmf for 
    \(Y\) given that \(X = 1\) is done in a similar way, and again they are the same. 
    Both pmfs can be found in be found in Table \ref{q02-tab}.
\end{itemize}

%' ============================================================================================================================================================
\section{Question 3} \noindent
We are considering points \((x,y)\) uniformly selected within an ellipse given by the equation
% \begin{align*}
%     \left( \frac{x}{a} \right)^2 + \left( \frac{y}{b} \right)^2 = 1,
% \end{align*}
\((x/a)^2 + (y/b)^2 = 1\), where \(a,b > 0\). 
Therefore, the probability of selecting a point \((x,y)\) from this region is \(f(x,y) = c\) for 
% \(-a \le x \le a\) and \(-b\sqrt{1 - (x/a)^2} \le y \le b\sqrt{1 - (x/a)^2}\), and \(f(x,y) = 0\) elsewhere. 
\(-a\le x \le a\), \(-b \le y \le b\), and \((x/a)^2 + (y/b)^2 \le 1\), and \(f(x,y) = 0\) elsewhere. 
To find \(c\), we use the fact that a joint pdf must integrate to 1 over all values of \(x\) and \(y\), so 
\begin{align*}
    1 = \int_{-a}^{a} \int_{-b\sqrt{1 - (x/a)^2}}^{b\sqrt{1 - (x/a)^2}} c \;\mathrm{d}y \;\mathrm{d}x
    = c \cdot 2 \int_{-a}^a b\sqrt{1 - (x/a)^2} \;\mathrm{d}x
    = c \cdot \pi a b,
\end{align*}
which implies that \(c = 1 / \pi a b\). The last equality is from the fact that the area of an ellipse is \(\pi a b\), which is what the integral is computing. 
To find the marginal density of \(X\) (\(Y\)), we integrate over all possible values of \(Y\) (\(X\)):
\begin{align*}
    f_X(x) &= \int_{-b\sqrt{1 - (x/a)^2}}^{b\sqrt{1 - (x/a)^2}} \frac{1}{\pi a b} \;\mathrm{d}y
    = \frac{y}{\pi ab} \bigg|_{-b\sqrt{1 - (x/a)^2}}^{b\sqrt{1 - (x/a)^2}} = \frac{2 \sqrt{1 - (x/a)^2}}{\pi a}
    ~~\text{for \(-a \le x \le a\)}, \\
    f_Y(y) &= \int_{-a\sqrt{1 - (y/b)^2}}^{a\sqrt{1 - (y/b)^2}} \frac{1}{\pi a b} \;\mathrm{d}x
    = \frac{x}{\pi ab} \bigg|_{-a\sqrt{1 - (y/b)^2}}^{a\sqrt{1 - (y/b)^2}} = \frac{2 \sqrt{1 - (y/b)^2}}{\pi b}
    ~~\text{for \(-b \le y \le b\)}.
\end{align*}
These results make sense intuitively. For \(f_X(x)\), we can see that the highest probability is obtained when \(x = 0\). At \(x=0\), the 
height of the ellipse is the greatest, so there is more ``width'' for \(x = 0\) to be chosen. And as \(x \to \pm a\), \(f_X(x) \to 0\), meaning that the closer
you get to the end of the ellipse, the less likely that point is to be chosen. 

% put table here? for now
\begin{table}
    \centering
    \def\arraystretch{1.25}
    % \begin{tabular}[ht]{|c|cccc|} \hline
    %     \(_X\backslash^Y\) & 1 & 2 & 3 & 4 \\\hline
    %     1 & 0.10 & 0.05 & 0.02 & 0.02 \\
    %     2 & 0.05 & 0.20 & 0.05 & 0.02 \\
    %     3 & 0.02 & 0.05 & 0.20 & 0.04 \\
    %     4 & 0.02 & 0.02 & 0.04 & 0.10 \\\hline
    % \end{tabular}
    % \vspace{1em}

    \begin{tabular}[b]{|c|cccc|} \hline
        \(x\) & 1 & 2 & 3 & 4 \\\hline
        \(f_X(x)\) & 0.19 & 0.32 & 0.31 & 0.18 \\
        \(f_{X|Y}(x|1)\) & 10/19 & 5/19 & 2/19 & 2/19 \\\hline
    \end{tabular}
    \quad
    \begin{tabular}[b]{|c|cccc|} \hline
        \(y\) & 1 & 2 & 3 & 4 \\\hline
        \(f_Y(y)\) & 0.19 & 0.32 & 0.31 & 0.18 \\
        \(f_{Y|X}(y|1)\) & 10/19 & 5/19 & 2/19 & 2/19 \\\hline
    \end{tabular}
    \caption{Information for question 2.}
    \label{q02-tab}
\end{table}

%' ============================================================================================================================================================
\section{Question 4} \noindent
The joint cdf of \(X\) and \(Y\) is given by \(F(x,y) = \big( 1 - \mathrm{e}^{-\alpha x} \big) \big( 1 - \mathrm{e}^{-\beta y} \big)\) for \(x,y \ge 0\) and
\(F(x,y) = 0\) elsewhere, where \(\alpha, \beta > 0\) are fixed constants. 
\begin{itemize}
    \item[(a)] \(X\) and \(Y\) are independent. This is because we can express the joing cdf as \(F(x,y) = G(x)\cdot H(y)\), where \(G(x) = 1 - \mathrm{e}^{-\alpha x}\)
    and \(H(y) = 1 - \mathrm{e}^{-\beta y}\). 
    \item[(b)] The joint pdf is given by 
    \begin{align*}
        f(x,y) = \frac{\partial^2 F}{\partial x \partial y} 
        = \frac{\partial}{\partial x}\big( 1 - \mathrm{e}^{-\alpha x} \big) \cdot \frac{\partial}{\partial y}\big( 1 - \mathrm{e}^{-\beta y} \big)
        = \alpha \mathrm{e}^{-\alpha x} \cdot \beta \mathrm{e}^{-\beta y}
        % = \alpha \beta \mathrm{e}^{-\alpha x - \beta y}
    \end{align*}
    for \(x, y \ge 0\) and \(f(x,y) = 0\) elsewhere. The marginal densities are given by 
    \begin{align*}
        f_X(x) 
        % &= \lim_{\phi \to \infty}\int_{0}^{\phi} \alpha \beta \mathrm{e}^{-\alpha x - \beta y} \;\mathrm{d}y
        % &= \lim_{\phi \to \infty}\int_{0}^{\phi} \alpha \mathrm{e}^{-\alpha x} \cdot \beta \mathrm{e}^{-\beta y} \;\mathrm{d}y
        &= \int_{0}^{\infty} \alpha \mathrm{e}^{-\alpha x} \cdot \beta \mathrm{e}^{-\beta y} \;\mathrm{d}y
        = \alpha \mathrm{e}^{-\alpha x} \cdot \lim_{\phi \to \infty} \left[- \mathrm{e}^{-\beta y} \right]_0^{\phi}
        = \alpha \mathrm{e}^{-\alpha x}, \\
        f_Y(y) 
        % &= \lim_{\phi \to \infty}\int_{0}^{\phi} \alpha \beta \mathrm{e}^{-\alpha x - \beta y} \;\mathrm{d}x
        % &= \lim_{\phi \to \infty}\int_{0}^{\phi} \alpha \mathrm{e}^{-\alpha x} \cdot \beta \mathrm{e}^{-\beta y} \;\mathrm{d}x
        &= \int_{0}^{\infty} \alpha \mathrm{e}^{-\alpha x} \cdot \beta \mathrm{e}^{-\beta y} \;\mathrm{d}x
        = \beta \mathrm{e}^{-\beta y} \cdot \lim_{\phi \to \infty} \left[- \mathrm{e}^{-\alpha x} \right]_0^{\phi}
        = \beta \mathrm{e}^{-\beta y}.
    \end{align*}
    Here we see that \(f_X(x)\cdot f_Y(y) = \alpha \mathrm{e}^{-\alpha x} \cdot \beta \mathrm{e}^{-\beta y} = f(x,y)\), another indication 
    that \(X\) and \(Y\) are independent. 
\end{itemize}

%' ============================================================================================================================================================
\section{Question 5} \noindent
\mycolab{Melissa Wu, Mariko Sawada}
Let \(X\) and \(Y\) have a joint pdf \(f(x,y) = c(x^2 - y^2)\mathrm{e}^{-x}\) for \(x \le 0\) and \(-x \le y \le x\).
To help with some of the integrations, we make use of the following result:
\begin{align*}
    \Omega(\theta, n) \coloneqq \int_{\theta}^{\infty} x^n \mathrm{e}^{-x} \;\mathrm{d}x = n! \cdot \mathrm{e}^{-\theta} \sum_{k=0}^{n} \frac{\theta^k}{k!}
\end{align*}
for all \(\theta \in \mathbb{R}\) and \(n \in \mathbb{Z}_{\ge0}\). This specific integral came up so many times in this question that 
I thought it was worth figuring out a general formula for it. 
% I came up with this while doing the assignment;
% a proof by induction can be found in Appendix A. 
A derivation of this result can be found in Appendix A. 
\begin{itemize}
    \item[(a)] 
    % This part will make use of the fact that \(\int_0^{\infty} x^n \mathrm{e}^{-x}\;\mathrm{d}x = n!\) for all \(n \in \mathbb{Z}_{\ge 0}\).
    % A proof of this claim can be found in Appendix A. %, and a more intuitive derivation can be found % link GitHub.
    Using the fact that the joint pdf must integrate to \(1\), we have 
    \begin{align*}
        1 &= \int_0^{\infty} \int_{-x}^{x} c \big( x^2 - y^2 \big) \mathrm{e}^{-x} \;\mathrm{d}y \;\mathrm{d}x
        = c \int_0^{\infty} \mathrm{e}^{-x} \left[ x^2y - \frac{y^3}{3} \right]_{-x}^{x} \;\mathrm{d}x \\
        &= \frac{4c}{3} \cdot \int_0^{\infty} x^3 \mathrm{e}^{-x} \;\mathrm{d}x
        = \frac{4c \cdot \Omega(0,3)}{3} 
        = \frac{4c \cdot 3!}{3}
        = 8 c,
    \end{align*}
    and so \(c = 1/8\). 
    \item[(b)] \(X\) and \(Y\) are not independent since the joint pdf does not have rectangular support. 
    \item[(c)] The region of support can be seen in Figure \ref{q05-fig}.
    The support for \(X\) and \(Y\) can be viewed two ways: all points such that \(x \ge 0\) and \(-x \le y \le x\),
    or all points such that \(y \in \mathbb{R}\) and \(|y| \le x < \infty\). 
    % Figure shows the region of integration for 
    \begin{figure}
        \centering
        \includestandalone[width = 0.25\textwidth]{img/q05-region-x}
        \hspace{3em}
        \includestandalone[width = 0.25\textwidth]{img/q05-region-y}
        \caption{The region of support for \(f(x,y) = (x^2 - y^2)\mathrm{e}^{-x}/8\).}
        \label{q05-fig}
    \end{figure}
    % When considering all possible \(x \ge 0\) values, we see that we must have \(-x \le y \le x\). 
    The marginal density of \(X\) is given by 
    \begin{align*}
        f_X(x) = \int_{-x}^x \frac{(x^2 - y^2)\mathrm{e}^{-x}}{8} \;\mathrm{d}y
        = \frac{\mathrm{e}^{-x}}{8} \left[ x^2y - \frac{y^3}{3} \right]_{-x}^x
        = \frac{e^{-x}}{8} \cdot \frac{4 x^3}{3}
        = \frac{x^3 \mathrm{e}^{-x}}{6}
    \end{align*}
    for \(x \ge 0\) and \(f_X(x) = 0\) otherwise.
    % When considering all possible \(-\infty < y < \infty\), we must split the support into two regions: where \(y \ge 0\) and 
    % where \(y < 0\). When \(y \ge 0\), we have \(0 \le y \le x < \infty\), so we have \(y \le x < \infty\). In this region, the marginal density 
    % is given by 
    % \begin{align*}
    %     f_Y(y) &= \int_y^{\infty} \int_{-x}^{x} \frac{\big( x^2 - y^2 \big) \mathrm{e}^{-x}}{8} \;\mathrm{d}x
    %     = \frac{1}{8} \left( \int_y^{\infty} x^2 \mathrm{e}^{-x} \;\mathrm{d}x - y^2 \int_y^{\infty} \mathrm{e}^{-x} \;\mathrm{d}x \right) \\
    %     &= \frac{1}{8} \big( \Omega(y,2) - y^2 \cdot \Omega(y,0) \big)
    %     = \frac{1}{8} \big( \mathrm{e}^{-y}(2 + 2y + y^2) - y^2\mathrm{e}^{-y} \big)
    %     = \frac{(1 + y) e^{-y}}{4}.
    % \end{align*}
    % In the second region, where \(y < 0\), we have \(0 > y \ge -x > - \infty\), and so we can have \(-y < x < \infty\). 
    % % By letting \(\tilde{y} = -y\) (and so \(\mathrm{d}\tilde{y} = - \mathrm{d}y\)),
    % The marginal density is this region is given by 
    % \begin{align*}
    %     f_Y(y) &= \int_{-y}^{\infty} \frac{\big( x^2 - y^2 \big) \mathrm{e}^{-x}}{8} \;\mathrm{d}x
    %     = \frac{1}{8} \left( \int_{-y}^{\infty} x^2 \mathrm{e}^{-x} \;\mathrm{d}x - y^2 \int_y^{\infty} \mathrm{e}^{-x} \;\mathrm{d}x \right) \\
    %     &= \frac{1}{8} \big( \Omega(-y,2) - y^2 \cdot \Omega(-y,0) \big)
    %     = \frac{1}{8} \big( \mathrm{e}^{y}(2 - 2y + y^2) - y^2\mathrm{e}^{y} \big)
    %     = \frac{(1 - y) e^{y}}{4}.
    % \end{align*}
    % As a whole, the joint density is given by 
    % \begin{align*}
    %     f_Y(y) = \begin{dcases}
    %         % (1 + y)\mathrm{e}^{-y} / 4 & \text{when \(y \ge 0\)}, \\
    %         % (1 - y)\mathrm{e}^{y} / 4  & \text{when \(y < 0\)}. \\
    %         \frac{(1 + y)\mathrm{e}^{-y}}{4} & \text{when \(y \ge 0\)}, \\
    %         \frac{(1 - y)\mathrm{e}^{y}}{4}  & \text{when \(y < 0\)}. \\
    %     \end{dcases}
    % \end{align*}
    The marginal density of \(Y\) is given by 
    \begin{align*}
        f_Y(y) &= \int_{|y|}^{\infty} \frac{\big( x^2 - y^2 \big) \mathrm{e}^{-x}}{8} \;\mathrm{d}x
        = \frac{1}{8} \left( \int_{|y|}^{\infty} x^2 \mathrm{e}^{-x} \;\mathrm{d}x - y^2 \int_{|y|}^{\infty} \mathrm{e}^{-x} \;\mathrm{d}x \right) \\
        &= \frac{1}{8} \Big( \Omega(|y|,2) - y^2 \cdot \Omega(|y|,0) \Big)
        = \frac{1}{8} \Big( 2\mathrm{e}^{-|y|}(1 + |y| + |y|^2/2) - y^2\mathrm{e}^{-|y|} \Big) \\
        &= \frac{1}{8} \Big( 2\mathrm{e}^{-|y|} + 2|y|\mathrm{e}^{-|y|} + y^2\mathrm{e}^{-|y|} - y^2\mathrm{e}^{-|y|} \Big)
        = \frac{(1 + |y|) e^{-|y|}}{4}
    \end{align*}
    for all \(y \in \mathbb{R}\).
    \item[(d)] The conditional distrubution for \(X\) given \(Y\) is 
    % \(f_{X|Y}(x|y)\) is found by dividing \(f(x,y)\) by \(f_Y(y)\), so 
    % By doing this, the conditional distrubutions are seen to be 
    \begin{align*}
        % f_{X|Y}(x|y) = \begin{dcases}
        %     % \frac{(x^2 - y^2)\mathrm{e}^{-x}/8}{(1+y)\mathrm{e}^{-y}/4}
        %     \frac{(x^2 - y^2)\mathrm{e}^{y-x}}{2(1+y)},    & y \ge 0,~  y \le x < \infty \\
        %     \frac{(x^2 - y^2)\mathrm{e}^{-(y+x)}}{2(1-y)}, & y < 0,~    -y < x < \infty
        % \end{dcases}
        f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)}
        = \frac{\frac{1}{8}(x^2 - y^2)\mathrm{e}^{-x}}{\frac{1}{4}(1 + |y|)\mathrm{e}^{-|y|}}
        = \frac{(x^2 - y^2)\mathrm{e}^{-(x + |y|)}}{2(1+|y|)}
    \end{align*}
    for \(|y| \le x < \infty\) and \(f_{X|Y}(x|y) = 0\) elsewhere. 
    Similarly, the conditional distribution for \(Y\) given \(X\) is 
    \begin{align*}
        f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)}
        = \frac{\frac{1}{8}(x^2 - y^2)\mathrm{e}^{-x}}{\frac{1}{6}x^3 \mathrm{e}^{-x}}
        = \frac{3(x^2 - y^2)}{4x^3}
    \end{align*}
    for \(-x \le y \le x\) and \(f_{Y|X}(y|x) = 0\) elsewhere. 
\end{itemize}

%' ============================================================================================================================================================
\section{Question 6} \noindent
Suppose the random variable \(X\) has some density \(f_X(x)\) and let \(Y = aX + b\) for some \(a \neq 0\), meaning \(Y\) is a linear combination of \(X\). 
We see that \(g(X) = aX + b\) is monotonic with inverse \(h(Y) = \frac{Y - b}{a}\) and \(h'(Y) = 1/a\). Then the density of \(Y\) is given by 
\begin{align*}
    f_Y(y) = f_X \big( h(y) \big) \cdot \big| h'(y) \big|
    = f_X\left( \frac{y - b}{a} \right) \cdot \frac{1}{|a|} = \frac{y}{|a|}.
\end{align*}

%' ============================================================================================================================================================
\section{Question 7} \noindent
Let \(F(x)\) be the cdf of some unspecified discrete random variable,
% Since e know that \(0 \le F(x) \le 1\). 
and let \(U \sim \mathrm{Unif}[0,1]\).
The cdf for \(U\) is given by \(\tilde{F}(u) = u\) for \(0 \le u \le 1\). 
If we define the random variable \(Y = k\) if \(F(k-1) < U \le F(k)\), then 
\begin{align*}
    \mathrm{Pr}(Y \le k) = \mathrm{Pr} \big( U \le F(k) \big) = \tilde{F} \big( F(k) \big) = F(k),
\end{align*}
and so the cdf of \(Y\) is actually given by \(F(k)\). 

%' ============================================================================================================================================================
\section{Question 8} \noindent
Let \(X\) be the number of cars in the left lane at a randomly chosen red light, where \(x = \{0,1,\ldots,7\}\). 
The engineer also believes \(f(x) \propto (x+1)(8-x)\).
\begin{itemize}
    \item[(a)] The engineer's assumption implies that \(f(x) = c(x+1)(8-x)\) for some \(c\). So 
    \begin{align*}
        1 &= \sum_{x=0}^7 c (x+1)(x-8)
        = c \sum_{i=0}^7 \big( 8 + 7x - x^2 \big)
        = c \left( 8 \sum_{i=0}^7 1 + 7 \sum_{i=0}^7 x - \sum_{i=0}^7 x^2 \right) \\
        &= c \left( 8\cdot 8 + 7 \cdot \frac{7 \cdot 8}{2} - \frac{7 \cdot 8 \cdot 15}{6} \right)
        = 120c,
    \end{align*}
    which means \(c = 120\) and \(f(x) = (x+1)(x-8)/120\). 
    \item[(b)] We have 
    \begin{align*}
        \mathrm{Pr}(X \ge 5) = \sum_{x=5}^7 \frac{(x+1)(8-x)}{120}
        = \frac{6\cdot 3 + 7 \cdot 2 + 8 \cdot 1}{120} = \frac{40}{120} = \frac{1}{3}.
    \end{align*}
\end{itemize}

%' ============================================================================================================================================================
\section{Question 9} \noindent
Let \(X\) have a pdf given by \(f(x) = x/8\) for \(0 \le x \le 4\).
The cdf is then given by
\begin{align*}
    F(x) = \int_0^x \frac{t}{8} \;\mathrm{d}t
    = \frac{t^2}{16} \bigg|_0^x
    = \frac{x^2}{16}
\end{align*}
for \(0 \le x \le 4\), \(F(x) = 0\) if \(x < 0\), and \(F(x) = 1\) if \(x > 4\). 
\begin{itemize}
    \item[(a)] We have \(\mathrm{Pr}(X \le t) = F(t) = t^2 / 16 \overset{\text{set}}{=} 1/4\), and solving for \(t\) gives us \(t = 2\). 
    \item[(b)] We have \(\mathrm{Pr}(X \ge t) = 1 - F(t) = (16 - t^2)/16 \overset{\text{set}}{=} 1/2\), and solving for \(t\) gives us \(t = 2\sqrt{2}\).
\end{itemize}

%' ============================================================================================================================================================
\section{Question 10} \noindent
Let \(X\) and \(Y\) be random variables such that \(0 \le x \le 3\) and \(0 \le y \le 4\). Within this rectangular region, let the joint cdf be 
\(F(x,y) = xy(x^2+y)/156\). From this we can also determine the joint cdf outside the region of joint support. To start we know that \(F(x,y) = 0\) if
\(x < 0\) or \(y < 0\). When \(0 \le X \le 3\) and \(y > 4\) (beyond the support of \(Y\)), we have \(F(x,y) = F_X(x)\), and when 
\(0 \le Y \le 4\) but \(X > 3\) (beyond the support of \(X\)), we have \(F(x,y) = F_Y(y)\); these quantities are given by 
\begin{align*}
    F_X(x) &= \lim_{y \to 4} F(x,y) = F(x,4) = \frac{x(x^2+y)}{39}, \\
    F_Y(y) &= \lim_{x \to 3} F(x,y) = F(3,y) = \frac{y(9 + y)}{52}.
\end{align*}
\begin{itemize}
    \item[(a)] We have 
    \begin{align*}
        \mathrm{Pr}&(1 \le X \le 2 \cap 1 \le Y \le 2)
        = F(2,2) - F(2,1) - F(1,2) + F(1,1) \\
        &= \frac{2 \cdot 2 (4 + 2)}{156} + \frac{1 \cdot 2 (1 + 2)}{156} + \frac{2 \cdot 1 (4 + 1)}{156} + \frac{1 \cdot 1 (1 + 1)}{156}
        = \frac{24 - 6 - 10 + 2}{156}
        = \frac{5}{78}.
    \end{align*}
    \item[(b)] We have 
    \begin{align*}
        \mathrm{Pr}&(2 \le X \le 4 \cap 2 \le Y \le 4) 
        = F(4,4) - F(4,2) - F(2,4) + F(2,2) \\
        &= \frac{4 (9 + 4)}{52} + \frac{2 (9 + 2))}{52} + \frac{2 \cdot 4 (4 + 4)}{156} + \frac{2 \cdot 2 (4 + 2)}{156}
        = \frac{156 - 66 - 64 + 24}{156}
        = \frac{25}{78}.
    \end{align*}
    \item[(c)] The cdf of \(Y\) is given by \(F_Y(y) = y(9 + y)/52\) for \(0 \le Y \le 4\), \(F_Y(y) = 0\) if \(y < 0\), and \(F_Y(y) = 1\) if \(y > 4\).
    \item[(d)] The joint pdf of \(X\) and \(Y\) is given by 
    \begin{align*}
        f(x,y) = \frac{\partial^2 F(x,y)}{\partial x \partial y} 
        = \frac{\partial}{\partial x} \frac{\partial}{\partial y} \frac{xy(x^2 + y)}{156} 
        = \frac{\partial}{\partial x} \frac{x(x^2 + y) + xy}{156}
        = \frac{3x^2 + 2y}{156}
    \end{align*}
    when \(0 \le x \le 3\) and \(0 \le y \le 4\), and \(f(x,y) = 0\) outside of the joint support. 
    \item[(e)] We have 
    \begin{align*}
        \mathrm{Pr}(Y \le X)
        &= \int_0^3 \int_0^x \frac{3x^2 + 2y}{156} \;\mathrm{d}y \;\mathrm{d}x
        = \frac{1}{156} \int_0^3 \big[ 3 x^2 y + y^2 \big]_0^x \;\mathrm{d}x 
        = \frac{1}{156} \int_0^3 \big( 3x^3 + x^2 \big) \;\mathrm{d}x \\
        &= \frac{1}{156} \left[ \frac{3x^4}{4} + \frac{x^3}{3} \right]_0^3
        = \frac{1}{156} \left( \frac{3 \cdot 81}{4} + \frac{27}{3} \right)
        = \frac{243 + 36}{156 \cdot 4} 
        = \frac{93}{208}.
    \end{align*}
\end{itemize}

%' ============================================================================================================================================================
\section{Appendix A} \noindent
Here we will prove the result used throughout question 5. By using integration by parts with \(u = x^n\) and \(\mathrm{d}v = \mathrm{e}^{-x}\), we have 
\begin{align*}
    \int x^n \mathrm{e}^{-x} \;\mathrm{d}x = -x^n \mathrm{e}^{-x} + n \int x^{n-1} \mathrm{e}^{-x} \;\mathrm{d}x + C.
\end{align*}
The next integral must also be solved via integration by parts, with \(u = x^{n-1}\) and \(\mathrm{d}v = \mathrm{e}^{-x}\):
\begin{align*}
    \int x^n \mathrm{e}^{-x} \;\mathrm{d}x &= -x^n \mathrm{e}^{-x} + n \left( -x^{n-1} \mathrm{e}^{-x} + (n-1) \int x^{n-2} \mathrm{e}^{-x} \;\mathrm{d}x  \right) + C \\
    &= -x^n \mathrm{e}^{-x} -nx^{n-1} \mathrm{e}^{-x} - n(n-1) \int x^{n-2} \mathrm{e}^{-x} \;\mathrm{d}x + C.
\end{align*}
% Noticing that \(n(n-1)\cdots(n-k)\)
By now a pattern starts to appear. We have to repeat the integration by parts for the right most integral \(n\) times in total. 
% In doing so, we can see that the \(j\)th term in this sum takes the form \(n(n-1)\cdots()\)
Completing this pattern and factoring out an \(n!\) from each term yields
\begin{align*}
    \int x^n \mathrm{e}^{-x} \;\mathrm{d}x &= -x^n \mathrm{e}^{-x} - nx^{n-1}\mathrm{e}^{-x} - n(n-1)x^{n-2}\mathrm{e}^{-x} - \cdots - n! + C\\
    &= -\mathrm{e}^{-x} \left( \frac{n!}{n!}x^{n} + \frac{n!}{(n-1)!}x^{n-1} + \frac{n!}{(n-2)!}x^{n-2} + \cdot + \frac{n!}{0!}x^0 \right) + C\\
    &= -n! \cdot \mathrm{e}^{-x} \left( \frac{x^n}{n!} + \frac{x^{n-1}}{(n-1)!} + \frac{x^{n-2}}{(n-2)!} + \cdot + \frac{x^0}{0!} \right) + C
    = -n! \cdot \mathrm{e}^{-x} \sum_{k=0}^n \frac{x^k}{k!} + C.
\end{align*}
% where \(C\) is the integration constant. 
Evaluating this integral from \(\theta\) to \(\infty\) gives us 
\begin{align*}
    \int_{\theta}^{\infty} x^n \mathrm{e}^{-x} \;\mathrm{d} 
    = \left[ -n! \cdot \mathrm{e}^{-x} \sum_{k=0}^n \frac{x^k}{k!} \right]_{\theta}^{\infty}
    = n! \cdot \mathrm{e}^{-\theta} \sum_{k=0}^n \frac{\theta^k}{k!} - n! \lim_{\phi \to \infty} \mathrm{e}^{-\phi} \sum_{k=0}^n \frac{\phi^k}{k!},
\end{align*}
and using L'Hopital's rule \(n\) times on the right limit shows that 
\begin{align*}
    \lim_{\phi \to \infty} \mathrm{e}^{-\phi} \sum_{k=0}^n \frac{\phi^k}{k!} = \lim_{\phi \to \infty} (-1)^n \mathrm{e}^{-\phi} = 0.
\end{align*}
which gives us our desired result. \hfill \(\square\)

\end{document}